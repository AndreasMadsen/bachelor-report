%!TEX root = ../Thesis.tex
\chapter{Introduction}

When a news story is covered by a single newspaper it is rare that it gives a full overview of the situation and enlighten all perspectives. Thus people will specific interrest or general curiosity may want to research the story further. However it can be difficult to find related articles, for example because the different perspective may result in a much different title. The reader may also want to avoid doing detail reading of unrelated articles or articles with the same perspective. To give a better experience to the reader, a search system deigned to find articles related to the same story could solve this issue.

Such a search system could cluster the articles such that each cluster contains all articles related to a single story. In order for this to work each article would have to be represented with some numbers (a vector), the numbers would be calculated such that articles about the same story have approximately the same vectors. A clustering algorithm would the be able to use these vectors to find articles about the same story. It is the task of calculating these vector representations of articles that is the main focus in this thesis.

The methods for finding the vector representations should extend to other data sources, such as patient diagnostics reports or drafts for legal acts.
Once the individual stories have been isolated, more detailed question can be asked and analyzed. Such as which counties there show interrest in a particular story, what kind of political perspective exists and does the amount of attention change over time. 

Finding vector representations of documents is called a well known subject and is properly the application in the field of natural language processing. However many of the current methods are based on the bag-of-word method of initially representing a document. Other methods such as LSA or LDA then transform this bag-of-word representation into the more useful document vectors.

The bag-of-word method works by counting how many times all word in predefined vocabulary appears in each document. The result is a long list of numbers, it will have many zeros as many words will not be used at all in some document. The problem with this strategy is that it ignores the order of words. For example ``China declares war on Japan'' is has a different meaning than ``Japan declares war on China'', but the bag-of-word representation of the sentences is the same because the words appear an equal amount of times.

Fortunately a new set of models is emerging, these models don't use the bag-of-word method to represent documents but instead scans over the words in the document. Because of this they are able to take the word order into account when creating the vector representations. It is some of these models that is discussed and compared in this theis.

The first model is called \textit{skip-gram} and creates vector representations of the individual words. It has been shown to be very good at capturing the semantic meaning, such that algebraic expressions such as $man - king + women = queen$ holds \cite{word2vec-comparing, word2vec-details}. Unfortunately it provides no direct way of obtaining a vector for the entire document. To solve this the \textit{paragraph2vec} model has been invented, it works in a similar way as the \textit{skip-gram} model but produces simultaneously vector representation of documents.

These two models are conceptually very simple, as limited to find log-linear patterns. A much more advanced method is the Sutskever method, the original paper \cite{sutskever} used it to provide state of the art text translation from english to french. As a side effect of this it also produces vector representations that contains the non-language specific meaning of the translated document. In this thesis that method is adapted to find vector representations of news articles. As translation data doesn't exists for the news article use case, the model is trained to predict the title given the article subhead.
