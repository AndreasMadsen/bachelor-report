%!TEX root = ../Thesis.tex
\chapter{Conclusion}

In this thesis 3 models for creating document vectors have been analyzed for the purpose of clustering articles related to the same story. While this is a very specific application the performance of the models will be related to their ability to create good vector representations of documents.

\paragraph{Skip-gram} The skip-gram model which is the most primitive and oldest of the three model, actually turned out to produce surprisingly good document vectors. It is not that the document vector are amazing and solves the problem well, but given that the purpose of the skip-gram model is to find good word vectors and not document vectors the good performance is surprisingly good. It is possible that a more clever weighted sum could have improved the result further, for example weighs nouns higher. But as the paragraph2vec paper \cite{doc2vec} also states the performance will always be limited by the fact that the purpose of the model is to find word vectors not document vectors.

\paragraph{paragraph2vec} While the skip-gram model was surprisingly good the paragraph2vec was surprisingly bad. According to the paragraph2vec paper \cite{doc2vec} this model should be better than the usual bag-of-word models and should produce a much better document vector than just taking the mean of the word vectors. Unfortunately this does not seams to be the case. A possible explanation is that the document vectors that the model produces doesn't represent the semantic meaning very well but rather the mood or political point of view of the document. The reasoning here is that semantic meaning is likely found the short context which is may best be predicted by the surrounding words, while the mood is found in the larger context which is where the document vectors help in the prediction. It is also entirely possible that the model work better with short documents like sentences and using the full article text isn't the optimal choice. While that is said the paragraph2vec paper \cite{doc2vec} does claim to work on documents of any length. The effect of the document length could be investigated more.

\paragraph{Sutskever} The Sutskever model did unfortunately not converge, why this is the case is unknown. But there are some indications that it is a vanishing gradient problem. If this is the case, then getting model to work will unfortunately in terms of implementation be quite complicated. The weight gradients became easily not-a-number for no apparent reason. This indicates the implementation is not numerically stable and indeed Theano does have issues with numerically stability, particular when involving \texttt{scan}, \texttt{softmax} and \texttt{log} \cite{theano-issue}. If these issues where solved it is likely that the implementation of the Sutskever model would converge. In the original paper \cite{sutskever} they implemented their own version from scratch in order to use 8 GPUs in parallel, likely they have also used various of tricks to make the implementation more numerical stable than what Theano can currently do.

If the Sutskever model would converge properly and give good title predictions given the article lead, then it is reasonable to expect the document vector would capture the meaning of the article. Whether or not those vectors would be better than what bag-of-words methods can produce is unknown.

\paragraph{Future work} If the Sutskever model predicts the article title well given the lead, but still doesn't produce good document vectors then one would have to look into different strategies. For example Instead of predicting the title given the article lead one could try an autoencoder, which would reconstruct the full document. It is also possible that more advanced clustering method are required in order to achieve good results.

However it is entirely possible that the problem can't be solved using unsupervised methods without getting a high error rate. In that case semi-supervised methods may be a good compromise. Manually labeling all stories in just 100000 articles certainly seams like an impractical huge task.
 