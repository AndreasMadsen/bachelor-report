%!TEX root = ../Thesis.tex
\chapter{Conclusion}

In this thesis 3 models for creating document vectors have been analyzed for the purpose of clustering articles related to the same story. While this is a very specific application, the performance of the models relates to their ability to create good vector representations of documents in general.

\paragraph{Skip-gram} The skip-gram model which is the most primitive and oldest of the three model, actually turned out to produce surprisingly good document vectors. It is not that the document vector are amazing and solves the problem well. But given that the purpose of the skip-gram model is to find good word vectors and not document vectors the performance is surprisingly good. It is possible that a more clever weighted sum could improve the result further, for example by weighting nouns higher. But as the paragraph2vec paper \cite{doc2vec} also states, the performance will always be limited by the fact that the purpose of the model is to find word vectors and not document vectors.

\paragraph{paragraph2vec} While the skip-gram model was surprisingly good, the paragraph2\-vec was surprisingly bad. According to the paragraph2vec paper \cite{doc2vec}, this model should be better than the usual bag-of-word models and should produce a much better document vector, than just taking the mean of the word vectors. Unfortunately this does not seem to be the case. A possible explanation is that the document vectors that the model produces doesn't represent the semantic meaning very well, but rather the mood or political point of view of the document. The reasoning here is that semantic meaning is likely found in the short context, which may best be predicted by the surrounding words, while the mood is found in the larger context which is where the document vectors help in the prediction. It is also entirely possible that the model work better with short documents like sentences, thus using the full article text isn't the optimal choice. While that is said the paragraph2vec paper \cite{doc2vec}, does claim to work on documents of any length. The effect of the document length should be investigated more.

\paragraph{Sutskever} The Sutskever model did unfortunately not converge, why this is the case is unknown. There are some indications that it is a vanishing gradient problem. Using a less na√Øve initialization or rectified linear units may solve this problem. However it may be more complicated than that. The weight gradients became easily not-a-number for no apparent reason. This indicates the implementation is not numerical stable and indeed Theano does have issues with numerical stability, particular when involving \texttt{scan}, \texttt{softmax} and \texttt{log} together \cite{theano-issue}. If these issues where solved it is likely that the implementation of the Sutskever model would converge. In the original paper \cite{sutskever} they implemented their own version from scratch in order to use 8 GPUs in parallel. Likely they have also used various tricks, to make the implementation more numerical stable than what Theano can currently do.

It is worth remembering that the implementation actually do work. The implementation was verified by constructing a synthetic problem, with have $9^8$ possible input and output sequences. However the training set was much smaller than this (1280 observation) and both test and training curves converged to zero error. This means the network is not only able to memorize an output given a input, but actually generalize really well. It is just that for complex problems, getting the initialization right is hard. However there is no reason to think it isn't possible to get the initialization right and thereby having it solve the real problem as well.

If the Sutskever model would converge probably and give good title predictions given the article lead, then it is reasonable to expect the document vector would capture the meaning of the article. Whether or not those vectors would be better than what bag-of-words methods can produce is unknown. If this is the case it could be because of the long sequences. In the original paper \cite{sutskever} it was shown that the model started to perform poorly for long sequences. In this thesis letters was used for input encoding thus adding to the sequence length.

\paragraph{Future work} If the Sutskever model predicts the article title well given the lead, but still doesn't produce good document vectors, then one would have to look into different strategies. For example, instead of predicting the title given the article lead, one could try an autoencoder, which would reconstruct the full document. It is also possible that more advanced clustering method are required in order to achieve good results.

However it is entirely possible that the problem can't be solved using unsupervised methods, without getting a high error rate. In that case semi-supervised methods may be a good compromise. Manually labeling all stories in just 100000 articles certainly seem like an impractical huge task.
 